{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOe1A8QYqa4V8zUyrwfQEMV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303a52332/Generative_AI_2025/blob/main/2303A52332_GENAI_N0_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS7LZsrVxTRl",
        "outputId": "d6de3dfd-5e88-4894-cedc-8dedfa620fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x that minimizes f(x) is: 0.001161634189870079\n"
          ]
        }
      ],
      "source": [
        "def f(x):\n",
        "    return 5 * x*4 + 3 * x*2 + 10\n",
        "def df(x):\n",
        "    return 20 * x**3 + 6 * x\n",
        "def gradient_descent_f(learning_rate=0.001, iterations=1000):\n",
        "    x = 1\n",
        "    for i in range(iterations):\n",
        "        grad = df(x)\n",
        "        x = x - learning_rate * grad\n",
        "    return x\n",
        "x_min_f = gradient_descent_f()\n",
        "print(f\"The value of x that minimizes f(x) is: {x_min_f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def g(x, y):\n",
        "    return 3 * x**2 + 5 * math.exp(-y) + 10\n",
        "def dg_dx(x, y):\n",
        "    return 6 * x\n",
        "def dg_dy(x, y):\n",
        "    return -5 * math.exp(-y)\n",
        "def gradient_descent_g(learning_rate=0.01, iterations=1000):\n",
        "    x, y = 10, 10\n",
        "    for i in range(iterations):\n",
        "        grad_x = dg_dx(x, y)\n",
        "        grad_y = dg_dy(x, y)\n",
        "        x = x - learning_rate * grad_x\n",
        "        y = y - learning_rate * grad_y\n",
        "    return x, y\n",
        "x_min_g, y_min_g = gradient_descent_g()\n",
        "print(f\"The values of x and y that minimize g(x, y) are: x = {x_min_g}, y = {y_min_g}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBsZs6aZxuZT",
        "outputId": "621db961-ca1d-4b6c-c010-adc5fb402623"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The values of x and y that minimize g(x, y) are: x = 1.3423123924933693e-26, y = 10.002267426506178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def z(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "def dz_dx(x):\n",
        "    sigmoid = 1 / (1 + math.exp(-x))\n",
        "    return sigmoid * (1 - sigmoid)\n",
        "\n",
        "def gradient_descent_z(learning_rate=0.01, iterations=1000):\n",
        "    x = 10\n",
        "    for i in range(iterations):\n",
        "        grad = dz_dx(x)\n",
        "        x = x - learning_rate * grad\n",
        "    return x\n",
        "\n",
        "\n",
        "x_min_z = gradient_descent_z()\n",
        "print(f\"The value of x that minimizes z(x) is: {x_min_z}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU81t9uWx0JJ",
        "outputId": "a6e1cd78-5a6f-4c96-e540-f0fb719bbb53"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x that minimizes z(x) is: 9.9995459389649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = [1, 2, 3, 4, 5]\n",
        "Y = [2, 4, 6, 8, 10]\n",
        "\n",
        "def predicted_output(x, M, C):\n",
        "    return M * x + C\n",
        "\n",
        "def square_error(M, C):\n",
        "    total_error = 0\n",
        "    for x, y in zip(X, Y):\n",
        "        y_pred = predicted_output(x, M, C)\n",
        "        total_error += (y - y_pred)**2\n",
        "    return total_error\n",
        "\n",
        "\n",
        "def grad_se_M(M, C):\n",
        "    grad = 0\n",
        "    for x, y in zip(X, Y):\n",
        "        grad += -2 * x * (y - (M * x + C))\n",
        "    return grad\n",
        "\n",
        "def grad_se_C(M, C):\n",
        "    grad = 0\n",
        "    for x, y in zip(X, Y):\n",
        "        grad += -2 * (y - (M * x + C))\n",
        "    return grad\n",
        "\n",
        "\n",
        "def gradient_descent_se(learning_rate=0.01, iterations=1000):\n",
        "    M, C = 0, 0\n",
        "    for i in range(iterations):\n",
        "        grad_M = grad_se_M(M, C)\n",
        "        grad_C = grad_se_C(M, C)\n",
        "        M = M - learning_rate * grad_M\n",
        "        C = C - learning_rate * grad_C\n",
        "    return M, C\n",
        "M_optimal, C_optimal = gradient_descent_se()\n",
        "print(f\"The optimal values of M and C that minimize the square error are: M = {M_optimal}, C = {C_optimal}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qhLEuvcx_pE",
        "outputId": "07a1e955-acd4-4a2b-bed3-c2ae2c594e9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal values of M and C that minimize the square error are: M = 1.9999999943842544, C = 2.0274623625998433e-08\n"
          ]
        }
      ]
    }
  ]
}